{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "    <a href=\"https://skills.network\" target=\"_blank\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"200\" alt=\"Skills Network Logo\"  />\n",
    "    </a>\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lab: Building Advanced Transformers**\n",
    "\n",
    "**Estimated time needed:  30 minutes**  \n",
    "\n",
    "In this lab, you will implement and experiment with advanced Transformer models using Keras. \n",
    "\n",
    "**Learning objectives:** \n",
    "\n",
    "By the end of this lab, you will: \n",
    "\n",
    "- Implement advanced Transformer models using Keras. \n",
    "\n",
    "- Apply Transformers to real-world sequential data tasks. \n",
    "\n",
    "- Build, train, and evaluate Transformer models. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step-by-Step Instructions: \n",
    "\n",
    "### Step 1: Import necessary libraries \n",
    "\n",
    "Before you start, you need to import the required libraries: TensorFlow and Keras. Keras is included within TensorFlow as `tensorflow.keras.`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow pyarrow \n",
    "%pip install pandas  \n",
    "%pip install scikit-learn \n",
    "%pip install matplotlib \n",
    "%pip install requests\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 00:56:40.461985: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-12-18 00:56:40.464327: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-18 00:56:40.468328: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-18 00:56:40.488357: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1734483400.507725      82 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1734483400.513326      82 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-18 00:56:40.534282: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import tensorflow as tf \n",
    "import requests\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Setup the Environment to generate synthetic stock price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synthetic stock_prices.csv created and loaded.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Create a synthetic stock price dataset\n",
    "np.random.seed(42)\n",
    "data_length = 2000  # Adjust data length as needed\n",
    "trend = np.linspace(100, 200, data_length)\n",
    "noise = np.random.normal(0, 2, data_length)\n",
    "synthetic_data = trend + noise\n",
    "\n",
    "# Create a DataFrame and save as 'stock_prices.csv'\n",
    "data = pd.DataFrame(synthetic_data, columns=['Close'])\n",
    "data.to_csv('stock_prices.csv', index=False)\n",
    "print(\"Synthetic stock_prices.csv created and loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (1899, 100, 1)\n",
      "Shape of Y: (1899,)\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset \n",
    "data = pd.read_csv('stock_prices.csv') \n",
    "data = data[['Close']].values \n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for training\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, Y = [], []\n",
    "\n",
    "    for i in range(len(data)-time_step-1):\n",
    "        a = data[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(data[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 100\n",
    "X, Y = create_dataset(data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X:\", X.shape) \n",
    "print(\"Shape of Y:\", Y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "`tensorflow` is the main library for machine learning in Python.  \n",
    "\n",
    "`stock_prices.csv` is the data set that is loaded. \n",
    "\n",
    "`MinMaxScaler` method is used to normalize the data.  \n",
    "\n",
    "`create_dataset`method is used to prepare the data for training. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Implement Multi-Head Self-Attention \n",
    "\n",
    "Define the Multi-Head Self-Attention mechanism. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttention(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    "\n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The MultiHeadSelfAttention layer implements the multi-head self-attention mechanism, which allows the model to focus on different parts of the input sequence simultaneously. \n",
    "\n",
    "- The attention parameter computes the attention scores and weighted sum of the values. \n",
    "\n",
    "- The split_heads parameter splits the input into multiple heads for parallel attention computation. \n",
    "\n",
    "- The call method applies the self-attention mechanism and combines the heads. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Implement Transformer block \n",
    "\n",
    "Define the Transformer block. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code:\n",
    "\n",
    "- The TransformerBlock layer combines multi-head self-attention with a feed-forward neural network and normalization layers.  \n",
    "\n",
    "- Dropout is used to prevent overfitting. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network with residual connections and layer normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Encoder Layer \n",
    "\n",
    "Define the Encoder layer. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(Layer): \n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(EncoderLayer, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    "\n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The EncoderLayer is similar to the TransformerBlock but is a reusable layer in the Transformer architecture. \n",
    "\n",
    "- It consists of a MultiHeadSelfAttention mechanism followed by a feedforward neural network. \n",
    "\n",
    "- Both sub-layers have residual connections around them, and layer normalization is applied to the output of each sub-layer. \n",
    "\n",
    "- The call method applies the self-attention, followed by the feedforward network, with residual connections and layer normalization. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement Transformer encoder \n",
    "\n",
    "Define the Transformer Encoder. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-18 01:15:51.065660: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.layers import Layer, Dense, LayerNormalization, Dropout \n",
    "\n",
    "class MultiHeadSelfAttention(Layer): \n",
    "    def __init__(self, embed_dim, num_heads=8): \n",
    "        super(MultiHeadSelfAttention, self).__init__() \n",
    "        self.embed_dim = embed_dim \n",
    "        self.num_heads = num_heads \n",
    "        self.projection_dim = embed_dim // num_heads \n",
    "        self.query_dense = Dense(embed_dim) \n",
    "        self.key_dense = Dense(embed_dim) \n",
    "        self.value_dense = Dense(embed_dim) \n",
    "        self.combine_heads = Dense(embed_dim) \n",
    " \n",
    "\n",
    "    def attention(self, query, key, value): \n",
    "        score = tf.matmul(query, key, transpose_b=True) \n",
    "        dim_key = tf.cast(tf.shape(key)[-1], tf.float32) \n",
    "        scaled_score = score / tf.math.sqrt(dim_key) \n",
    "        weights = tf.nn.softmax(scaled_score, axis=-1) \n",
    "        output = tf.matmul(weights, value) \n",
    "        return output, weights \n",
    "\n",
    "\n",
    "    def split_heads(self, x, batch_size): \n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.projection_dim)) \n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3]) \n",
    "\n",
    "\n",
    "    def call(self, inputs): \n",
    "        batch_size = tf.shape(inputs)[0] \n",
    "        query = self.query_dense(inputs) \n",
    "        key = self.key_dense(inputs) \n",
    "        value = self.value_dense(inputs) \n",
    "        query = self.split_heads(query, batch_size) \n",
    "        key = self.split_heads(key, batch_size) \n",
    "        value = self.split_heads(value, batch_size) \n",
    "        attention, _ = self.attention(query, key, value) \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3]) \n",
    "        concat_attention = tf.reshape(attention, (batch_size, -1, self.embed_dim)) \n",
    "        output = self.combine_heads(concat_attention) \n",
    "        return output \n",
    "\n",
    "class TransformerBlock(Layer): \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerBlock, self).__init__() \n",
    "        self.att = MultiHeadSelfAttention(embed_dim, num_heads) \n",
    "        self.ffn = tf.keras.Sequential([ \n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim), \n",
    "        ]) \n",
    "\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6) \n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6) \n",
    "        self.dropout1 = Dropout(rate) \n",
    "        self.dropout2 = Dropout(rate) \n",
    " \n",
    "\n",
    "    def call(self, inputs, training): \n",
    "        attn_output = self.att(inputs) \n",
    "        attn_output = self.dropout1(attn_output, training=training) \n",
    "        out1 = self.layernorm1(inputs + attn_output) \n",
    "        ffn_output = self.ffn(out1) \n",
    "        ffn_output = self.dropout2(ffn_output, training=training) \n",
    "        return self.layernorm2(out1 + ffn_output) \n",
    "\n",
    "class TransformerEncoder(Layer): \n",
    "    def __init__(self, num_layers, embed_dim, num_heads, ff_dim, rate=0.1): \n",
    "        super(TransformerEncoder, self).__init__() \n",
    "        self.num_layers = num_layers \n",
    "        self.embed_dim = embed_dim \n",
    "        self.enc_layers = [TransformerBlock(embed_dim, num_heads, ff_dim, rate) for _ in range(num_layers)] \n",
    "        self.dropout = Dropout(rate) \n",
    "\n",
    "    def call(self, inputs, training=False): \n",
    "        x = inputs \n",
    "        for i in range(self.num_layers): \n",
    "            x = self.enc_layers[i](x, training=training) \n",
    "        return x \n",
    "\n",
    "# Example usage \n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "inputs = tf.random.uniform((1, 100, embed_dim)) \n",
    "outputs = transformer_encoder(inputs, training=False)  # Use keyword argument for 'training' \n",
    "print(outputs.shape)  # Should print (1, 100, 128) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The TransformerEncoder is composed of multiple TransformerBlock layers, implementing the encoding part of the Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Build and Compile the Transformer model \n",
    "\n",
    "Integrate the Transformer Encoder into a complete model for sequential data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │       <span style=\"color: #00af00; text-decoration-color: #00af00\">793,088</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerEncoder</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12800</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,801</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer_4 (\u001b[38;5;33mInputLayer\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_48 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │           \u001b[38;5;34m256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_encoder_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │       \u001b[38;5;34m793,088\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerEncoder\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12800\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_49 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │        \u001b[38;5;34m12,801\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">806,145</span> (3.08 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m806,145\u001b[0m (3.08 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the necessary parameters \n",
    "\n",
    "embed_dim = 128 \n",
    "num_heads = 8 \n",
    "ff_dim = 512 \n",
    "num_layers = 4 \n",
    "\n",
    "# Define the Transformer Encoder \n",
    "transformer_encoder = TransformerEncoder(num_layers, embed_dim, num_heads, ff_dim) \n",
    "\n",
    "# Build the model \n",
    "input_shape = (X.shape[1], X.shape[2]) \n",
    "inputs = tf.keras.Input(shape=input_shape) \n",
    "\n",
    "# Project the inputs to the embed_dim \n",
    "x = tf.keras.layers.Dense(embed_dim)(inputs) \n",
    "encoder_outputs = transformer_encoder(x) \n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "outputs = tf.keras.layers.Dense(1)(flatten) \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Summary of the model \n",
    "model.summary() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The Transformer Encoder model defines the necessary parameters, flattens the output, and ends with a dense layer to produce the final output.  \n",
    "\n",
    "- The model is then compiled with the Adam optimizer and mean squared error loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Train the Transformer model \n",
    "\n",
    "Train the model on the prepared dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 1s/step - loss: 12.6530\n",
      "Epoch 2/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 994ms/step - loss: 0.2241\n",
      "Epoch 3/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1468\n",
      "Epoch 4/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1944\n",
      "Epoch 5/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1290\n",
      "Epoch 6/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1588\n",
      "Epoch 7/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - loss: 0.1147\n",
      "Epoch 8/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1051\n",
      "Epoch 9/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0950\n",
      "Epoch 10/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - loss: 0.1209   \n",
      "Epoch 11/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.1463\n",
      "Epoch 12/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0879\n",
      "Epoch 13/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0848\n",
      "Epoch 14/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0858\n",
      "Epoch 15/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0880\n",
      "Epoch 16/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - loss: 0.0636\n",
      "Epoch 17/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0872\n",
      "Epoch 18/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0476\n",
      "Epoch 19/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0649\n",
      "Epoch 20/20\n",
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 1s/step - loss: 0.0478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x7f7abc348090>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "The model is trained on the normalized stock price data for 20 epochs with a batch size of 32. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Evaluate and Make Predictions \n",
    "\n",
    "Evaluate the model's performance and make predictions on the dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m60/60\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 297ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions \n",
    "predictions = model.predict(X) \n",
    "predictions = scaler.inverse_transform(predictions) \n",
    " \n",
    "\n",
    "# Plot the predictions \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.plot(data, label='True Data') \n",
    "plt.plot(np.arange(time_step, time_step + len(predictions)), predictions, label='Predictions') \n",
    "plt.xlabel('Time') \n",
    "plt.ylabel('Stock Price') \n",
    "plt.legend() \n",
    "plt.show() \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code: \n",
    "\n",
    "- The model's predictions are transformed back to the original scale using the inverse transform of the scaler. \n",
    "\n",
    "- The true data and predictions are plotted to visualize the model's performance. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises: \n",
    "\n",
    " ### Exercise 1: Add dropout to the Transformer model \n",
    "\n",
    " **Objective: Understand how to add dropout layers to the Transformer model to prevent overfitting.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Add a dropout layer after the Flatten layer in the model. \n",
    "\n",
    "- Set the dropout rate to 0.5. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n",
    "from tensorflow.keras.layers import Dropout \n",
    "\n",
    "# Add a dropout layer after the Flatten layer \n",
    "\n",
    "flatten = tf.keras.layers.Flatten()(encoder_outputs) \n",
    "dropout = Dropout(0.5)(flatten) \n",
    "outputs = tf.keras.layers.Dense(1)(dropout) \n",
    "\n",
    "# Build the model \n",
    "model = tf.keras.Model(inputs, outputs) \n",
    "\n",
    "# Compile the model \n",
    "model.compile(optimizer='adam', loss='mse') \n",
    "\n",
    "# Train the model \n",
    "\n",
    "model.fit(X, Y, epochs=20, batch_size=32)   \n",
    "\n",
    "# Evaluate the model \n",
    "\n",
    "loss = model.evaluate(X, Y) \n",
    "print(f'Test loss: {loss}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with different batch sizes \n",
    "\n",
    "**Objective: Observe the impact of different batch sizes on model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Train the model with a batch size of 16. \n",
    "\n",
    "- Train the model with a batch size of 64. \n",
    "\n",
    "- Compare the training time and performance. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n",
    "model.fit(X, Y, epochs=20, batch_size=16)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 16: {loss}')\n",
    "\n",
    "# Train the model with batch size 64\n",
    "model.fit(X, Y, epochs=20, batch_size=64)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with batch size 64: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Use a different activation function \n",
    "\n",
    " **Objective: Understand how different activation functions impact the model performance.** \n",
    "\n",
    " Instructions: \n",
    "\n",
    "- Change the activation function of the Dense layer to `tanh`. \n",
    "\n",
    "- Train and evaluate the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Write your code here.\n",
    "# Change the activation function of the Dense layer to tanh\n",
    "outputs = tf.keras.layers.Dense(1, activation='tanh')(flatten)\n",
    "\n",
    "# Build the model\n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, Y, epochs=20, batch_size=32)\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X, Y)\n",
    "print(f'Test loss with tanh activation: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulations on completing this lab! In this lab, you have built an advanced Transformer model using Keras and applied it to a time series forecasting task. You have learned how to define and implement multi-head self-attention, Transformer blocks, encoder layers, and integrate them into a complete Transformer model. By experimenting with different configurations and training the model, you can further improve its performance and apply it to various sequential data tasks. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "28ac4fd81c1d713f83dcd1cdf1d3383ad25ea92873288fe9e978e9a17b314709"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
